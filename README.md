# Проект асинхронного парсинга документации Python

## **Основная цель проекта**
Изучение возможностей фреймворка [Scrapy](https://docs.scrapy.org/en/latest/index.html).

##  **Описание проекта**
Выполняется парсинг данных со страницы с общей информацией о PEP (https://peps.python.org/), 
переход по ссылкам и сбор данных о каждом PEP.
Парсер подготавливает данные и сохраняет их в два файла формата ```csv``` в папку ```results```.

## **Запуск проекта**
Выполните следующие команды в терминале:

1. Клонировать проект из репозитория
```
git clone git@github.com:DoeryMK/scrapy_parser_pep.git
```
или
```
git clone https://github.com/DoeryMK/scrapy_parser_pep.git
```
2. Создать, активировать виртуальное окружение и в него установить зависимости:
```
python -m venv venv
```
```
source venv/Scripts/activate
```
```
pip install -r requirements.txt 
```
3. Запустить парсер из командной строки:
```
scrapy crawl pep
```

### _Вывод результатов_
Результатом работы парсера будет создание двух файлов:
1. ``pep_ДатаВремя.csv`` - содержит список всех PEP (номер, название и статус);
2. ``status_summary_ДатаВремя.csv`` - содержит сводку по статусам PEP: 
сколько найдено документов в каждом статусе (статус, количество). 
В последней строке этого файла в колонке ``Total`` выводится общее количество всех документов.


## Авторы: Мухамеджанова Д.С. ([DoeryMK](https://github.com/DoeryMK)) 